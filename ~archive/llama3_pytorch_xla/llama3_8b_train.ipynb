{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/aws_neuronx_venv_pytorch_2_1/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_13040/1057427633.py\u001b[0m(69)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     67 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     68 \u001b[0;31m    \u001b[0;31m# Fix the random number generator seeds for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 69 \u001b[0;31m    \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     70 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m    \u001b[0;31m# XLA: Specify XLA device (defaults to a NeuronCore on Trn1 instance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  next(iter(train_loader))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_13040/1057427633.py\u001b[0m(72)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m    \u001b[0;31m# XLA: Specify XLA device (defaults to a NeuronCore on Trn1 instance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 72 \u001b[0;31m    \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xla'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m    \u001b[0;31m# Move model to device and declare optimizer and loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  x = next(iter(train_loader))\n",
      "ipdb>  x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-2.1191,  0.0776,  3.4171,  ...,  0.7731, -0.6935, -0.8865],\n",
      "          [ 0.7186, -0.0556,  0.3989,  ..., -1.1913, -0.3478,  0.7789],\n",
      "          [-0.4001,  0.1024, -0.1736,  ..., -0.9343,  0.7738,  0.6558],\n",
      "          ...,\n",
      "          [ 1.2527,  0.4576, -1.2948,  ..., -2.0772,  0.2891,  1.7844],\n",
      "          [-1.1152, -0.8433,  0.2706,  ..., -1.4534, -1.0795, -0.4402],\n",
      "          [ 1.3365,  0.7099,  1.3206,  ..., -0.8835, -1.4699,  1.8144]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6372,  1.1949,  2.0840,  ...,  0.4330,  0.3890, -0.6343],\n",
      "          [-1.2200, -2.3117,  0.7436,  ..., -1.0024, -1.8883,  0.8620],\n",
      "          [-0.9815, -1.9794,  0.1981,  ...,  0.4779, -0.8568, -0.5031],\n",
      "          ...,\n",
      "          [ 2.3262,  0.9625,  2.0861,  ...,  1.3275,  0.1223, -0.3024],\n",
      "          [-0.9390,  0.8797, -0.6360,  ..., -0.6617, -0.6971, -0.5100],\n",
      "          [-0.0382, -0.5070,  1.0390,  ..., -0.2144, -0.2371,  0.5000]]],\n",
      "\n",
      "\n",
      "        [[[-1.4908, -0.3226,  1.1499,  ...,  0.4864, -1.3301,  0.6620],\n",
      "          [-0.8348,  1.8126, -0.4964,  ...,  1.2050,  0.8846,  1.1003],\n",
      "          [-0.3194,  0.8562,  0.2726,  ...,  0.2797, -0.1866,  0.7230],\n",
      "          ...,\n",
      "          [-1.1835,  0.4420, -0.1637,  ..., -0.1632,  1.9752, -0.1501],\n",
      "          [ 0.5072,  0.9663,  1.1321,  ...,  0.4770, -0.6863, -2.4363],\n",
      "          [-0.2705,  0.0401,  0.2151,  ...,  0.1845,  0.9541, -0.5290]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.3949, -0.5575,  0.3943,  ..., -1.5746, -0.9027, -1.1138],\n",
      "          [ 1.0605, -0.0045,  0.5676,  ..., -1.9646,  0.7717, -0.1620],\n",
      "          [-0.0706, -1.1491,  0.6261,  ...,  0.2251,  1.3762, -0.9773],\n",
      "          ...,\n",
      "          [ 0.7692, -0.6792,  1.2458,  ..., -0.7450,  0.8355,  0.6855],\n",
      "          [ 0.9218, -0.7789,  1.6973,  ...,  0.6272, -0.4057, -0.6813],\n",
      "          [ 0.8276,  0.7348, -0.6114,  ...,  1.3200,  0.0070,  0.9542]]],\n",
      "\n",
      "\n",
      "        [[[-0.0970, -1.2564,  0.4486,  ..., -0.3396,  1.0514, -3.6188],\n",
      "          [ 0.2443, -0.8362, -2.1740,  ..., -0.6418,  0.8365,  0.8091],\n",
      "          [-1.2213,  0.7164, -0.0316,  ..., -0.1092,  0.1089,  0.2597],\n",
      "          ...,\n",
      "          [ 0.3757, -0.0699,  1.5768,  ...,  0.3949,  0.7642,  1.3542],\n",
      "          [-1.9396,  0.4181,  0.0401,  ..., -0.1383, -0.6821,  0.8527],\n",
      "          [ 0.5009, -0.4192,  0.1606,  ..., -2.2003,  0.3076, -0.2223]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5651,  0.6216, -0.2596,  ...,  0.8835, -1.1499, -1.3406],\n",
      "          [ 0.2028,  2.6012, -0.1286,  ...,  0.0481,  0.6111, -1.1641],\n",
      "          [ 0.6167,  1.7408,  1.3158,  ..., -0.8075,  0.3341,  1.6984],\n",
      "          ...,\n",
      "          [-0.2038,  0.6559,  0.3126,  ..., -0.0666, -0.3869, -1.7231],\n",
      "          [ 0.1993,  0.5962,  1.6752,  ..., -0.0479, -0.6222, -0.6456],\n",
      "          [ 0.5820,  0.7545,  0.0443,  ..., -1.6273, -0.3199,  0.3653]]]]), tensor([2, 1, 0, 0, 8, 5, 6, 6, 6, 3, 4, 7, 9, 5, 7, 4, 7, 6, 3, 1, 1, 0, 2, 5,\n",
      "        5, 7, 5, 5, 1, 9, 7, 6]))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Training ---------------\n",
      "2024-12-01 03:59:46.000119:  13040  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-12-01 03:59:46.000121:  13040  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/45b0d0d6-e9f1-4140-9abd-0955882e46a2/model.MODULE_4027976367933384269+d7517139.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/45b0d0d6-e9f1-4140-9abd-0955882e46a2/model.MODULE_4027976367933384269+d7517139.neff --verbose=35\n",
      "..\n",
      "Compiler status PASS\n",
      "2024-12-01 04:00:08.000856:  13040  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-12-01 04:00:08.000857:  13040  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/74c80928-6ed8-440c-89b3-a6c1cec7dfa3/model.MODULE_9384584493755895831+d7517139.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/74c80928-6ed8-440c-89b3-a6c1cec7dfa3/model.MODULE_9384584493755895831+d7517139.neff --verbose=35\n",
      ".\n",
      "Compiler status PASS\n",
      "Train throughput (iter/sec): 9.648080391193824\n",
      "Final loss is 2.3690\n",
      "Train throughput (iter/sec): 1053.9730689912137\n",
      "Final loss is 2.3364\n",
      "Train throughput (iter/sec): 1069.7402576843585\n",
      "Final loss is 2.3052\n",
      "Train throughput (iter/sec): 1074.3037218915051\n",
      "Final loss is 2.2759\n",
      "----------End Training ---------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "# Global constants\n",
    "EPOCHS = 4\n",
    "WARMUP_STEPS = 2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create dummy MNIST data\n",
    "def create_dummy_mnist_data(num_samples=1000):\n",
    "    # MNIST images are 28x28 pixels, labels are 0-9\n",
    "    dummy_data = torch.randn(num_samples, 1, 28, 28)\n",
    "    dummy_labels = torch.randint(0, 10, (num_samples,))\n",
    "    return dummy_data, dummy_labels\n",
    "\n",
    "class DummyDataLoader:\n",
    "    def __init__(self, data, labels, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(data)\n",
    "        self.current_idx = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.current_idx = 0\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.current_idx >= self.num_samples:\n",
    "            raise StopIteration\n",
    "            \n",
    "        end_idx = min(self.current_idx + self.batch_size, self.num_samples)\n",
    "        batch_data = self.data[self.current_idx:end_idx]\n",
    "        batch_labels = self.labels[self.current_idx:end_idx]\n",
    "        self.current_idx = end_idx\n",
    "        \n",
    "        return batch_data, batch_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "# Declare 3-layer MLP for MNIST dataset\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=28*28, output_size=10, layers=[120, 84]):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layers[0])\n",
    "        self.fc2 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc3 = nn.Linear(layers[1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def main():\n",
    "    # Create dummy data\n",
    "    dummy_data, dummy_labels = create_dummy_mnist_data()\n",
    "    \n",
    "    # Create dummy data loader\n",
    "    train_loader = DummyDataLoader(dummy_data, dummy_labels, BATCH_SIZE)\n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "    # Fix the random number generator seeds for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    # XLA: Specify XLA device (defaults to a NeuronCore on Trn1 instance)\n",
    "    device = 'xla'\n",
    "    \n",
    "    # Move model to device and declare optimizer and loss function\n",
    "    model = MLP().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    \n",
    "    # Run the training loop\n",
    "    print('----------Training ---------------')\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            train_x = train_x.view(train_x.size(0), -1)\n",
    "            train_x = train_x.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            output = model(train_x)\n",
    "            loss = loss_fn(output, train_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            xm.mark_step()  # XLA: collect ops and run them in XLA runtime\n",
    "            if idx < WARMUP_STEPS:  # skip warmup iterations\n",
    "                start = time.time()\n",
    "    \n",
    "        # Compute statistics for the last epoch\n",
    "        interval = idx - WARMUP_STEPS  # skip warmup iterations\n",
    "        throughput = interval / (time.time() - start)\n",
    "        print(f\"Train throughput (iter/sec): {throughput}\")\n",
    "        print(f\"Final loss is {loss.detach().to('cpu'):0.4f}\")\n",
    "    \n",
    "    # Save checkpoint for evaluation\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    xm.save(checkpoint, 'checkpoints/checkpoint.pt')\n",
    "    \n",
    "    print('----------End Training ---------------')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_TORCH'] = 'True'  # To use transformers library in TPU\n",
    "os.environ['XLA_USE_BF16'] = 'True'\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "import os\n",
    "import contextlib\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import contextlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.runtime as xr\n",
    "# xr.use_spmd()\n",
    "\n",
    "# # import torch_xla.distributed.spmd as xs\n",
    "# from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "# from torch_xla.experimental.xla_sharding import Mesh\n",
    "\n",
    "# # import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "# import torch_xla.distributed.parallel_loader as pl\n",
    "# import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xr.is_spmd()==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('')\n",
    "# model_partitioning = importlib.import_module('trainer_lib.model_partitioning')\n",
    "# importlib.reload(model_partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# This notebook can be used to train any of the 7B, 8B models. Check out the 80B notebook to train bigger model.\n",
    "supported_models = [\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-step-50K-105b\",\n",
    "]\n",
    "\n",
    "# Select a supported model from above list to use!\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "HUGGINGFACE_TOKEN = input(\"Please provide your HUGGINGFACE_TOKEN: \") # YOUR_HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LoRA config for your model.\n",
    "Use the below code to configure the LoRA config for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(*, model, lora_rank=None, lora_alpha=None, lora_dropout=None):\n",
    "    \"\"\"Applies LoRA configuration to the model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8 if not lora_rank else lora_rank,\n",
    "        lora_alpha=32 if not lora_alpha else lora_alpha,\n",
    "        lora_dropout=0.1 if not lora_dropout else lora_dropout,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(*, model_name, hugging_face_token):\n",
    "    \"\"\"Downloads and initializes the model.\"\"\"\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token\n",
    "    )\n",
    "\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # model = apply_lora(\n",
    "    #     model=model,\n",
    "    #     lora_rank=TRAINER_CONFIG[\"lora_rank\"],\n",
    "    #     lora_alpha=TRAINER_CONFIG[\"lora_alpha\"],\n",
    "    #     lora_dropout=TRAINER_CONFIG[\"lora_dropout\"],\n",
    "    # )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_spmd(*, model, mesh):\n",
    "    # Apply on layers within model.\n",
    "    model_partitioning_util.partition_model(model, mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure dataset pipeline for your model\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=1, seq_length=32, max_examples=None):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Tokenize the dataset.\n",
    "    def _tokenize(examples):\n",
    "        # Tokenized is list within list. Compute labels for causalLM by shifting input_id; \n",
    "        # consequently truncate input_id to penultimate position.\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=seq_length+1)\n",
    "        labels = tokenized['input_ids'].copy()\n",
    "        tokenized['labels'] = [label[1:] for label in labels]\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        return tokenized\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(max_examples))\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Now let's train the model. We are using PyTorch XLA's Fully Sharded Data Parallel (FSDP) to distribute the model across the 8 TPU cores available on TPU v3-8. This approach allows for efficient training on TPU hardware. We also utilize PyTorch/XLA's MpDeviceLoader to efficiently load data onto the TPU cores.\n",
    "\n",
    "**NOTE:** It's important to note that the **first step of training will be slow**. This is because XLA takes time initially to compile the computational graph. However, once the compilation is complete, subsequent steps will run much faster using compiled+cached graph, and leveraging the full power of the all TPU cores for accelerated training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_update(step,\n",
    "                          loss,\n",
    "                          epoch=None,\n",
    "                         ):\n",
    "    \"\"\"Prints the training metrics at a given step.\"\"\"\n",
    "    if xm.is_master_ordinal():  # Only print on the master device\n",
    "        update_data = [\n",
    "            'Training',\n",
    "            f'Epoch={epoch}' if epoch is not None else 0,\n",
    "            f'Step={step}',\n",
    "            f'Loss={loss:.5f}',\n",
    "        ]\n",
    "        print(' | '.join(item for item in update_data if item), flush=True)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = init_model(\n",
    "        model_name=MODEL_NAME, hugging_face_token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerConfig(epochs=1, batch_size=32, seq_length=64, learning_rate=0.0001, max_steps=100, max_examples=None, print_every_n_steps=5, lora_rank=8, lora_alpha=32, lora_dropout=0.1)\n"
     ]
    }
   ],
   "source": [
    "print(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(99)\n",
    "device = xm.xla_device()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=trainer_config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b88658b9c94599b8acba856d837b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f9378b53244ea5b132e45698d0d974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7764 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = get_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=trainer_config.batch_size,\n",
    "    seq_length=trainer_config.seq_length,\n",
    "    max_examples=trainer_config.max_examples,\n",
    ")\n",
    "# train_dataloader = pl.MpDeviceLoader(\n",
    "#     train_dataloader, \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# test_dataloader = pl.MpDeviceLoader(\n",
    "#     test_dataloader, \n",
    "#     device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "\n",
    "input_ids, attention_mask, labels = (\n",
    "    batch[\"input_ids\"],\n",
    "    batch[\"attention_mask\"],\n",
    "    batch[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(\n",
    "    input_ids=input_ids.to(\"xla\"), \n",
    "    attention_mask=attention_mask.to(\"xla\"), \n",
    "    labels=labels.to(\"xla\") if labels is not None else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9770, device='xla:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    ")\n",
    "loss = output.loss\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    global model, tokenizer, trainer_config\n",
    "\n",
    "    print(trainer_config)\n",
    "    \n",
    "    torch.manual_seed(99)\n",
    "    device = xm.xla_device()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create a mesh for the model partitioning.\n",
    "    # num_devices = xr.global_runtime_device_count()\n",
    "    # mesh_shape = (1, num_devices, 1)\n",
    "    # device_ids = np.array(range(num_devices))\n",
    "    # mesh = Mesh(device_ids, mesh_shape, (\"dp\", \"fsdp\", \"mp\"))\n",
    "        \n",
    "    # Partition the model using SPMD.\n",
    "    # model_partitioning.partition_model(model=model, mesh=mesh)\n",
    "    \n",
    "    # Configure the training loop.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=trainer_config.learning_rate)\n",
    "    \n",
    "    train_dataloader, test_dataloader = get_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=trainer_config.batch_size,\n",
    "        seq_length=trainer_config.seq_length,\n",
    "        max_examples=trainer_config.max_examples,\n",
    "    )\n",
    "    # train_dataloader = pl.MpDeviceLoader(\n",
    "    #     train_dataloader, \n",
    "    #     device\n",
    "    # )\n",
    "    \n",
    "    # test_dataloader = pl.MpDeviceLoader(\n",
    "    #     test_dataloader, \n",
    "    #     device\n",
    "    # )\n",
    "    \n",
    "    should_break = False\n",
    "    for epoch in range(trainer_config.epochs):\n",
    "        print(f\"Epoch {epoch} train begin..\")\n",
    "        tracker = xm.RateTracker()\n",
    "        \n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if trainer_config.max_steps is not None and step > trainer_config.max_steps:\n",
    "                should_break = True\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "            input_ids=input_ids.to(device)\n",
    "            attention_mask=attention_mask.to(device) \n",
    "            labels=labels.to(device) if labels is not None else None\n",
    "            # xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "            # xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "            # xs.mark_sharding(labels, mesh, (0, 1))\n",
    "            \n",
    "            output = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            # xm.mark_step()\n",
    "            \n",
    "            if step % trainer_config.print_every_n_steps == 0:\n",
    "                loss_cpu = loss.item()\n",
    "                xm.add_step_closure(\n",
    "                    print_training_update,\n",
    "                    args=(step, loss_cpu, epoch)\n",
    "                )\n",
    "        \n",
    "        # UNCOMMENT BELOW TO RUN EVAL.\n",
    "        # model.eval()\n",
    "        # eval_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for step, batch in enumerate(test_dataloader):\n",
    "        #         input_ids, attention_mask, labels = (\n",
    "        #             batch[\"input_ids\"],\n",
    "        #             batch[\"attention_mask\"],\n",
    "        #             batch[\"labels\"],\n",
    "        #         )\n",
    "        #         xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "        #         xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "        #         xs.mark_sharding(labels, mesh, (0, 1))\n",
    "        \n",
    "        #         output = model(\n",
    "        #             input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "        #         )\n",
    "        #         eval_loss += output.loss.item()\n",
    "        # avg_eval_loss = eval_loss / len(test_dataloader)\n",
    "        # xm.add_step_closure(\n",
    "        #     lambda: print(f\"Eval loss: {avg_eval_loss:.4f}\"),\n",
    "        # )\n",
    "        if should_break:\n",
    "            break\n",
    "    result = {'device': xm.get_ordinal(), 'loss': loss.item()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainerConfig:    \n",
    "    epochs: int = 1\n",
    "    batch_size: int = 32\n",
    "    seq_length: int = 64\n",
    "    \n",
    "    learning_rate: float = 1e-4\n",
    "\n",
    "    max_steps: int | None = 100\n",
    "    max_examples: int| None = None\n",
    "    \n",
    "    print_every_n_steps: int = 5\n",
    "    \n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "trainer_config = TrainerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainerConfig(epochs=1, batch_size=32, seq_length=64, learning_rate=0.0001, max_steps=100, max_examples=None, print_every_n_steps=5, lora_rank=8, lora_alpha=32, lora_dropout=0.1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d0985d2ddd49b38cdef58548a9b737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8464678876a44f87af180370910d0cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7764 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin..\n",
      "2024-12-01 04:39:20.000726:  13040  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-12-01 04:39:20.000745:  13040  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.15.128.0+56dc5a86/MODULE_4793052177575340561+d7517139/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-12-01 04:39:38.000951:  13040  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-12-01 04:39:38.000956:  13040  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --target=trn1 --framework=XLA /tmp/ubuntu/neuroncc_compile_workdir/1ad304dd-ddc9-412b-a3b0-7cbd8cf2847a/model.MODULE_15522692757083607861+d7517139.hlo_module.pb --output /tmp/ubuntu/neuroncc_compile_workdir/1ad304dd-ddc9-412b-a3b0-7cbd8cf2847a/model.MODULE_15522692757083607861+d7517139.neff --verbose=35\n",
      "............................................................"
     ]
    }
   ],
   "source": [
    "train(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# import time; start_time = time.time()\n",
    "# try:\n",
    "#     xmp.spawn(train, args=(), start_method=\"fork\")\n",
    "# except Exception as e:\n",
    "#     # Catch the expected error of obtaining results from multiple TPU chips when starting distributed training from a notebook.\n",
    "#     print()\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Execution time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model to HuggingFace Hub\n",
    "Uncoment the following cell to push the model to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "HUGGINGFACE_USERNAME = input(\"Please provide your HUGGINGFACE_USERNAME: \")\n",
    "\n",
    "model = model.cpu()\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(\"Uncomment below code if you want to upload to HF.\")\n",
    "# print(\"Uploading to HF...\")\n",
    "# merged_model.push_to_hub(\n",
    "#     f\"{HUGGINGFACE_USERNAME}/felafax-llama3-finetuned\",  # repo name\n",
    "#     tokenizer=tokenizer,\n",
    "#     private=False,\n",
    "#     create_pr=False,\n",
    "#     max_shard_size=\"2GB\",\n",
    "#     token=HUGGINGFACE_TOKEN,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
